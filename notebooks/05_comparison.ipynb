{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec58ddc5",
   "metadata": {},
   "source": [
    "# 05 - Comparison of Naive Bayes and LSTM Models\n",
    "\n",
    "In this notebook we:\n",
    "- load the saved evaluation reports of both models (`nb_report.txt` and `lstm_report.txt`)\n",
    "- parse the classification metrics into structured DataFrames\n",
    "- compare Naive Bayes and LSTM using accuracy, precision, recall and F1-scores\n",
    "- visualize overall performance differences between the models\n",
    "- compare class-wise F1-scores to understand strengths and weaknesses\n",
    "- summarize which model performs better and by how much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae024e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Make plots show inside the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "results_path = Path(\"../results\")\n",
    "\n",
    "nb_report_path = results_path / \"nb_report.txt\"\n",
    "lstm_report_path = results_path / \"lstm_report.txt\"\n",
    "\n",
    "with open(nb_report_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    nb_report_str = f.read()\n",
    "\n",
    "with open(lstm_report_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lstm_report_str = f.read()\n",
    "\n",
    "print(\"Naive Bayes classification report:\\n\")\n",
    "print(nb_report_str)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"LSTM classification report:\\n\")\n",
    "print(lstm_report_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05c04dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_classification_report(report_str):\n",
    "    \"\"\"\n",
    "    Parse sklearn.metrics.classification_report (string) into:\n",
    "    - df: DataFrame with index = label rows, columns = precision/recall/f1-score/support\n",
    "    - accuracy: float\n",
    "    \"\"\"\n",
    "    lines = [line.strip() for line in report_str.split(\"\\n\") if line.strip()]\n",
    "    rows = []\n",
    "    accuracy = None\n",
    "\n",
    "    for line in lines:\n",
    "        # Skip header line (starts with 'precision' etc.)\n",
    "        if line.startswith(\"precision\") or line.startswith(\"macro avg\") and \"avg\" not in line:\n",
    "            continue\n",
    "\n",
    "        tokens = line.split()\n",
    "\n",
    "        # Class rows: label precision recall f1 support\n",
    "        if len(tokens) == 5:\n",
    "            label = tokens[0]\n",
    "            precision, recall, f1, support = tokens[1:]\n",
    "            rows.append({\n",
    "                \"label\": label,\n",
    "                \"precision\": float(precision),\n",
    "                \"recall\": float(recall),\n",
    "                \"f1-score\": float(f1),\n",
    "                \"support\": int(support),\n",
    "            })\n",
    "\n",
    "        # 'accuracy' row: accuracy value support\n",
    "        elif tokens[0] == \"accuracy\":\n",
    "            # Typically: 'accuracy  0.97  1234'\n",
    "            if len(tokens) >= 2:\n",
    "                try:\n",
    "                    accuracy = float(tokens[1])\n",
    "                except ValueError:\n",
    "                    accuracy = None\n",
    "\n",
    "        # 'macro avg' or 'weighted avg'\n",
    "        elif tokens[0] in (\"macro\", \"weighted\") and len(tokens) == 6:\n",
    "            label = \" \".join(tokens[0:2])   # \"macro avg\" / \"weighted avg\"\n",
    "            precision, recall, f1, support = tokens[2:]\n",
    "            rows.append({\n",
    "                \"label\": label,\n",
    "                \"precision\": float(precision),\n",
    "                \"recall\": float(recall),\n",
    "                \"f1-score\": float(f1),\n",
    "                \"support\": int(support),\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows).set_index(\"label\")\n",
    "    return df, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e08a844",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_df, nb_accuracy = parse_classification_report(nb_report_str)\n",
    "lstm_df, lstm_accuracy = parse_classification_report(lstm_report_str)\n",
    "\n",
    "print(\"Naive Bayes parsed report:\")\n",
    "display(nb_df)\n",
    "print(f\"Naive Bayes accuracy: {nb_accuracy:.4f}\" if nb_accuracy is not None else \"Naive Bayes accuracy: N/A\")\n",
    "\n",
    "print(\"\\nLSTM parsed report:\")\n",
    "display(lstm_df)\n",
    "print(f\"LSTM accuracy: {lstm_accuracy:.4f}\" if lstm_accuracy is not None else \"LSTM accuracy: N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dddc0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_summary(df, accuracy, model_name):\n",
    "    \"\"\"\n",
    "    Extract overall metrics (accuracy, macro avg, weighted avg) from parsed df.\n",
    "    Returns a one-row DataFrame with index=model_name.\n",
    "    \"\"\"\n",
    "    row = {}\n",
    "\n",
    "    # Accuracy\n",
    "    row[\"accuracy\"] = accuracy\n",
    "\n",
    "    # Macro avg row\n",
    "    if \"macro avg\" in df.index:\n",
    "        row[\"macro_precision\"] = df.loc[\"macro avg\", \"precision\"]\n",
    "        row[\"macro_recall\"] = df.loc[\"macro avg\", \"recall\"]\n",
    "        row[\"macro_f1\"] = df.loc[\"macro avg\", \"f1-score\"]\n",
    "    else:\n",
    "        row[\"macro_precision\"] = None\n",
    "        row[\"macro_recall\"] = None\n",
    "        row[\"macro_f1\"] = None\n",
    "\n",
    "    # Weighted avg row\n",
    "    if \"weighted avg\" in df.index:\n",
    "        row[\"weighted_precision\"] = df.loc[\"weighted avg\", \"precision\"]\n",
    "        row[\"weighted_recall\"] = df.loc[\"weighted avg\", \"recall\"]\n",
    "        row[\"weighted_f1\"] = df.loc[\"weighted avg\", \"f1-score\"]\n",
    "    else:\n",
    "        row[\"weighted_precision\"] = None\n",
    "        row[\"weighted_recall\"] = None\n",
    "        row[\"weighted_f1\"] = None\n",
    "\n",
    "    return pd.DataFrame([row], index=[model_name])\n",
    "\n",
    "\n",
    "nb_summary = overall_summary(nb_df, nb_accuracy, \"Naive Bayes\")\n",
    "lstm_summary = overall_summary(lstm_df, lstm_accuracy, \"LSTM\")\n",
    "\n",
    "overall_comparison = pd.concat([nb_summary, lstm_summary])\n",
    "overall_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe95c0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll compare accuracy, macro F1 and weighted F1\n",
    "metrics_to_plot = [\"accuracy\", \"macro_f1\", \"weighted_f1\"]\n",
    "\n",
    "plot_df = overall_comparison[metrics_to_plot].copy()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plot_df.plot(kind=\"bar\")\n",
    "plt.title(\"Overall Performance Comparison\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(title=\"Metric\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c89875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only the \"real\" classes, i.e., rows that are not averages\n",
    "def class_rows(df):\n",
    "    return df[~df.index.isin([\"accuracy\", \"macro avg\", \"weighted avg\"])]\n",
    "\n",
    "nb_classes = class_rows(nb_df)\n",
    "lstm_classes = class_rows(lstm_df)\n",
    "\n",
    "# Build a combined DataFrame of F1 scores per class and per model\n",
    "all_labels = sorted(set(nb_classes.index) | set(lstm_classes.index))\n",
    "\n",
    "rows = []\n",
    "for label in all_labels:\n",
    "    nb_f1 = nb_classes.loc[label, \"f1-score\"] if label in nb_classes.index else None\n",
    "    lstm_f1 = lstm_classes.loc[label, \"f1-score\"] if label in lstm_classes.index else None\n",
    "    rows.append({\"label\": label, \"Naive Bayes\": nb_f1, \"LSTM\": lstm_f1})\n",
    "\n",
    "class_f1_df = pd.DataFrame(rows).set_index(\"label\")\n",
    "class_f1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587ff177",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "class_f1_df.plot(kind=\"bar\")\n",
    "plt.title(\"Class-wise F1-score Comparison\")\n",
    "plt.ylabel(\"F1-score\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(title=\"Model\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904ac95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "nb_acc = overall_comparison.loc[\"Naive Bayes\", \"accuracy\"]\n",
    "lstm_acc = overall_comparison.loc[\"LSTM\", \"accuracy\"]\n",
    "\n",
    "summary_lines = []\n",
    "\n",
    "summary_lines.append(f\"- Naive Bayes accuracy: **{nb_acc:.4f}**\")\n",
    "summary_lines.append(f\"- LSTM accuracy: **{lstm_acc:.4f}**\")\n",
    "\n",
    "macro_nb = overall_comparison.loc[\"Naive Bayes\", \"macro_f1\"]\n",
    "macro_lstm = overall_comparison.loc[\"LSTM\", \"macro_f1\"]\n",
    "\n",
    "summary_lines.append(f\"- Naive Bayes macro F1: **{macro_nb:.4f}**\")\n",
    "summary_lines.append(f\"- LSTM macro F1: **{macro_lstm:.4f}**\")\n",
    "\n",
    "text = \"### Summary\\n\\n\" + \"\\n\".join(summary_lines)\n",
    "Markdown(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
